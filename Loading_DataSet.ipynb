{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for reading the data files\n",
    "def build_data_frame(path):\n",
    "    rows = []\n",
    "    index = []\n",
    "    classification =[]\n",
    "    for file_name, text, classification in read_files(path):\n",
    "        rows.append({'text':text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "        \n",
    "    data_frame = pd.DataFrame(rows,index=index)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    newline=''\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        print('Root folder: {0}'.format(root))\n",
    "        print('Number of files read: {0}'.format(len(file_names)))\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(root,file_name)\n",
    "            if(os.path.isfile(file_path)):\n",
    "                #print(file_name)\n",
    "                if(\"D\" in file_name):\n",
    "                    label=\"D\"\n",
    "                elif(\"R\" in file_name):\n",
    "                    label=\"R\"\n",
    "                else:\n",
    "                    label=\"X\"\n",
    "                lines = []\n",
    "                f = open(file_path)\n",
    "                for line in f:\n",
    "                    lines.append(line.rstrip(\"\\n\"))\n",
    "                f.close()\n",
    "                content=newline.join(lines)\n",
    "                yield file_name, content, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root folder: /home/sadu/Documents/IE594-Data-Science/Speech-classification/data_set\n",
      "Number of files read: 856\n"
     ]
    }
   ],
   "source": [
    "# here I set the path of data set using os.getcwd()\n",
    "path = os.path.join(os.getcwd(), 'data_set')\n",
    "# intitalize the empty data frame\n",
    "data = pd.DataFrame({'text':[],'class':[]})\n",
    "# call the function to build the data set\n",
    "data=data.append(build_data_frame(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parse file name, reorder columns and print to csv\n",
    "\n",
    "file_name = pd.Series(data.index.values)\n",
    "bill, speaker, meta_date, file_name= file_name.str.split('_').str\n",
    "data['bill_id'] = bill.values\n",
    "data['speaker_id'] = speaker.values\n",
    "data['meta_date'] = meta_date.values\n",
    "data = data[['meta_date', 'speaker_id', 'class', 'bill_id', 'text']]\n",
    "data.to_csv('clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into train and test\n",
    "train_data, test_data = train_test_split(data, test_size = 0.2, stratify = data['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create N-gram features\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "counts = count_vectorizer.fit_transform(train_data['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41, 37],\n",
       "       [40, 54]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "targets = train_data['class'].values\n",
    "classifier.fit(counts, targets)\n",
    "#Test results\n",
    "test_counts = count_vectorizer.transform(test_data['text'])\n",
    "predictions = classifier.predict(test_counts)\n",
    "print(confusion_matrix(test_data['class'].values, predictions))\n",
    "classification_report(test_data['class'].values, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
